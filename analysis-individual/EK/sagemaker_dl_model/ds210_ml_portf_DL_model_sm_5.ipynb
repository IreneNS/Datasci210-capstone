{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3irOtxA7sI4H"
   },
   "source": [
    "# DS210 - Optimizing Portfolio to Adapt to Regime Change\n",
    "## Deep Learning Model - SageMaker Training Framework\n",
    "### Elaine edits marked as \"## ELAINE\" in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only includes SageMaker Framework code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:147\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:282\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    276\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:229\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    223\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(tf\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    224\u001b[0m process \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    225\u001b[0m     target\u001b[38;5;241m=\u001b[39m_wrap,\n\u001b[1;32m    226\u001b[0m     args\u001b[38;5;241m=\u001b[39m(fn, i, args, tf\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    227\u001b[0m     daemon\u001b[38;5;241m=\u001b[39mdaemon,\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m error_files\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    231\u001b[0m processes\u001b[38;5;241m.\u001b[39mappend(process)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train_model(rank, model_class, model_args, learning_rate, batch_size, train_dataset, val_dataset, \n",
    "                criterion, num_epochs,  patience, \n",
    "                device, is_distributed, world_size, train_loss_ll, val_loss_ll):\n",
    "\n",
    "    if is_distributed:\n",
    "        dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)  # PyTorch assigns ranks automatically\n",
    "        torch.cuda.set_device(rank)\n",
    "\n",
    "        # create model inside function \n",
    "        model = model_class(*model_args).to(rank)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
    "        # model = torch.nn.DataParallel(model, device_ids=[rank])\n",
    "        \n",
    "        train_sampler = torch.utils.data.DistributedSampler(train_dataset) if is_distributed else None\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False, sampler=train_sampler) #IN: make sure do not shuffle for time series data\n",
    "        val_sampler = torch.utils.data.DistributedSampler(val_dataset) if is_distributed else None\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False, sampler=val_sampler) #IN: make sure do not shuffle for time series data\n",
    "\n",
    "    else:\n",
    "        # create model inside function \n",
    "        model = model_class(**model_args) # if model_args is a list, use *, if model_args is a dic, use **\n",
    "        rank = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(rank)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False) #IN: make sure do not shuffle for time series data\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False) #IN: make sure do not shuffle for time series data\n",
    "\n",
    "    # Create optimizer inside function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    train_loss_l_process = []\n",
    "    val_loss_l_process = []\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if is_distributed:\n",
    "            train_sampler.set_epoch(epoch)  # Ensures proper shuffling per epoch\n",
    "\n",
    "        train_loss_ep = 0\n",
    "        val_loss_ep = 0 \n",
    "\n",
    "        for batch_idx, batch_item in enumerate(train_dataloader):\n",
    "            x_batch, y_batch = \\\n",
    "                batch_item['features'].to(rank), \\\n",
    "                    batch_item['target'].to(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            train_loss_ep +=loss.item()\n",
    "        \n",
    "        avg_train_loss_ep = train_loss_ep/len(train_dataloader)\n",
    "        print (f'Epoch {epoch+1}/{num_epochs}, Train loss: {avg_train_loss_ep:.3f}')\n",
    "        train_loss_l_process.append(avg_train_loss_ep)\n",
    "\n",
    "        # log validation loss\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_item in enumerate(val_dataloader):\n",
    "                x_batch, y_batch = \\\n",
    "                    batch_item['features'].to(rank), \\\n",
    "                        batch_item['target'].to(rank)\n",
    "                pred = model(x_batch)\n",
    "                loss = criterion(pred, y_batch)\n",
    "                val_loss_ep+=loss.item()\n",
    "        avg_val_loss_ep = val_loss_ep/len(val_dataloader)\n",
    "        val_loss_l_process.append(avg_val_loss_ep)\n",
    "\n",
    "        print (f'Epoch {epoch+1}/{num_epochs}, Val loss: {avg_val_loss_ep:.3f}')\n",
    "\n",
    "        # check if stop early\n",
    "        if avg_val_loss_ep < best_loss:\n",
    "            best_loss = avg_val_loss_ep \n",
    "            patience_counter = 0 # reset patience counter once improved\n",
    "            # best_model_state = model.state_dict() # mark best model\n",
    "            # best_model_state = model.module.state_dict() # mark best model (in DDP framework, save in a way that loading is as normal later)\n",
    "            best_model_state = model.module.state_dict() if is_distributed else model.state_dict()\n",
    "        else:\n",
    "            patience_counter +=1 \n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print ('Early stopping triggered')\n",
    "            break # stop training if validatio loss failed to keep reducing after > patience epochs\n",
    "    \n",
    "    # load best model weights\n",
    "    if best_model_state is not None:\n",
    "      model.load_state_dict(best_model_state)\n",
    "\n",
    "    # log results for this processor\n",
    "    train_loss_ll[rank] = train_loss_l_process\n",
    "    val_loss_ll[rank] = val_loss_l_process\n",
    "\n",
    "    # clear up\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # save the model\n",
    "    # # dl_model_directory = r'C:\\Mine\\U.S.-2019\\NPB living - 2 - related\\School-part time\\Berkeley-202308\\MIDS classes\\210-Capstone\\Project-related\\code-IN/dl_model/'\n",
    "    # dl_model_directory = f'/content/drive/MyDrive/Datasci-210/dl_model/{dl_model_checkpoint}/{rebal_freq}'\n",
    "    # model_filename = f'model_dl_att_2_{rank}.pth'  # Choose a filename for your model; model_dl_att_1\n",
    "    # torch.save(model.state_dict(), dl_model_directory+model_filename)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# Train model with logging - parallelism\n",
    "world_size = torch.cuda.device_count() \n",
    "manager = mp.Manager()\n",
    "train_loss_ll = manager.list([[] for _ in range(world_size)]) # Create shared list for losses for processor\n",
    "val_loss_ll = manager.list([[] for _ in range(world_size)])\n",
    "\n",
    "# launch processes\n",
    "model_dl_class = ReturnPredictionModel_Transformer\n",
    "model_args = {\n",
    "    'input_dim': len(feature_cols),\n",
    "    'lstm_dim': 64,\n",
    "    'lstm_layers': 10,\n",
    "    'transformer_layers': 3,\n",
    "    'num_heads': 8,\n",
    "    'attn_dropout': 0.1,\n",
    "    'forward_dim': 50,\n",
    "    'hidden_dims_l': [32, 16],\n",
    "    'dropout': 0.3,\n",
    "    'output_dim': len(tgt_cols)\n",
    "}\n",
    "\n",
    "# model_args = (\n",
    "#     len(feature_cols),\n",
    "#     64,\n",
    "#     10,\n",
    "#     3,\n",
    "#     8,\n",
    "#     0.1,\n",
    "#     50,\n",
    "#     [32, 16],\n",
    "#     0.3,\n",
    "#     len(tgt_cols)\n",
    "# )\n",
    "\n",
    "learning_rate = 0.0002\n",
    "batch_size = 48\n",
    "\n",
    "mp.spawn(train_model, args=(model_dl_class, model_args, learning_rate, batch_size, train_dataset_dl, val_dataset_dl,\n",
    "                criterion, num_epochs,  patience, \n",
    "                device, is_distributed, world_size, train_loss_ll, val_loss_ll),\n",
    "                nprocs=world_size)\n",
    "\n",
    "# process the results from all processors\n",
    "train_loss_l = pd.DataFrame(train_loss_ll).mean().tolist()\n",
    "val_loss_l = pd.DataFrame(val_loss_ll).mean().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and error\n",
    "args_list = [\n",
    "        ('--model_dir', {'type': str, 'default': 's3://capstone-general/NN-related/dl_model/', \n",
    "            'help':'put in model directory before model checkpoint name and rebal freq'}), \n",
    "        # ('--model_checkpoint', {'type': str, 'default': 'dl_model_checkpoint0', \n",
    "        #     'help':'put in model checkpoint name'}), \n",
    "        # ('--rebal_freq', {'type':str, 'default':'D', 'help':'put in rebal_freq'}),\n",
    "        ('--model_filename',{'type':str, 'default':'model_dl_gen_sm_0', 'help':'put in wanted model file name'}),\n",
    "        ('--batch_size', {'type':int, 'default': 48, 'help':'put in batch size for loader'}),\n",
    "        ('--learning_rate', {'type':float, 'default':0.0002, 'help':'put in learning rate'}),\n",
    "        ('--input_dim', {'type':int, 'default':9, 'help':'input_dim'}),\n",
    "        ('--lstm_dim', {'type':int, 'default':64, 'help':'lstm_dim'}),\n",
    "        ('--lstm_layers', {'type':int, 'default':5, 'help':'lstm_layers'}),\n",
    "        ('--transformer_layers', {'type':int, 'default':3, 'help':'transformer_layers'}),\n",
    "        ('--num_heads', {'type':int, 'default':4, 'help':'num_heads'}),\n",
    "        ('--attn_dropout', {'type':float, 'default':0.1, 'help':'attn_dropout'}),\n",
    "        ('--forward_dim', {'type':int, 'default':64, 'help':'forward_dim'}),\n",
    "        ('--hidden_dims_l', {'type':str, 'default':'[128,32]', 'help':'hidden_dims_l'}),\n",
    "        ('--dropout', {'type':float, 'default':0.3, 'help':'dropout'}),\n",
    "        ('--output_dim', {'type':int, 'default':1, 'help':'output_dim'}),\n",
    "        ('--num_epochs', {'type':int, 'default':30, 'help':'num_epochs'}),\n",
    "        ('--patience', {'type':int, 'default':5, 'help':'patience'}),\n",
    "        ('--seed', {'type':int, 'default':42, 'help':'seed'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Framework (in addition to what's already in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()  # what's this for?\n",
    "role = sagemaker.get_execution_role()\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-03-03-04-08-09-029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-03 04:08:09 Starting - Starting the training job\n",
      "2025-03-03 04:08:09 Pending - Training job waiting for capacity......\n",
      "2025-03-03 04:09:06 Pending - Preparing the instances for training...\n",
      "2025-03-03 04:09:40 Downloading - Downloading input data...\n",
      "2025-03-03 04:09:50 Downloading - Downloading the training image...............\n",
      "2025-03-03 04:12:37 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:57,916 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:57,957 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:57,971 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:57,973 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:57,973 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:59,024 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3fs in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.9/site-packages (from s3fs->-r requirements.txt (line 2)) (1.34.149)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from s3fs->-r requirements.txt (line 2)) (2024.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from botocore>=1.12.91->s3fs->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore>=1.12.91->s3fs->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore>=1.12.91->s3fs->-r requirements.txt (line 2)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:59,992 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:12:59,993 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,063 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,136 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,159 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,160 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,166 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,166 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,166 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,218 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,241 sagemaker-training-toolkit INFO     instance type: ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,242 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 4 num_processes: 4\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,291 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-03 04:13:00,315 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"attn_dropout\": 0.1,\n",
      "        \"batch_size\": 12,\n",
      "        \"dropout\": 0.3,\n",
      "        \"forward_dim\": 5,\n",
      "        \"hidden_dims_l\": \"[128,32]\",\n",
      "        \"input_dim\": 9,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"lstm_dim\": 64,\n",
      "        \"lstm_layers\": 5,\n",
      "        \"model_dir\": \"s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/\",\n",
      "        \"model_filename\": \"model_dl_trf_sm_0\",\n",
      "        \"num_epochs\": 30,\n",
      "        \"num_heads\": 4,\n",
      "        \"output_dim\": 1,\n",
      "        \"patience\": 5,\n",
      "        \"seed\": 42,\n",
      "        \"transformer_layers\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2025-03-03-04-08-09-029\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-851725363789/pytorch-training-2025-03-03-04-08-09-029/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"dl_train_trf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"dl_train_trf.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"attn_dropout\":0.1,\"batch_size\":12,\"dropout\":0.3,\"forward_dim\":5,\"hidden_dims_l\":\"[128,32]\",\"input_dim\":9,\"learning_rate\":0.0002,\"lstm_dim\":64,\"lstm_layers\":5,\"model_dir\":\"s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/\",\"model_filename\":\"model_dl_trf_sm_0\",\"num_epochs\":30,\"num_heads\":4,\"output_dim\":1,\"patience\":5,\"seed\":42,\"transformer_layers\":3}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=dl_train_trf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.g5.12xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=dl_train_trf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-851725363789/pytorch-training-2025-03-03-04-08-09-029/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.g5.12xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"attn_dropout\":0.1,\"batch_size\":12,\"dropout\":0.3,\"forward_dim\":5,\"hidden_dims_l\":\"[128,32]\",\"input_dim\":9,\"learning_rate\":0.0002,\"lstm_dim\":64,\"lstm_layers\":5,\"model_dir\":\"s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/\",\"model_filename\":\"model_dl_trf_sm_0\",\"num_epochs\":30,\"num_heads\":4,\"output_dim\":1,\"patience\":5,\"seed\":42,\"transformer_layers\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2025-03-03-04-08-09-029\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-851725363789/pytorch-training-2025-03-03-04-08-09-029/source/sourcedir.tar.gz\",\"module_name\":\"dl_train_trf\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"dl_train_trf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--attn_dropout\",\"0.1\",\"--batch_size\",\"12\",\"--dropout\",\"0.3\",\"--forward_dim\",\"5\",\"--hidden_dims_l\",\"[128,32]\",\"--input_dim\",\"9\",\"--learning_rate\",\"0.0002\",\"--lstm_dim\",\"64\",\"--lstm_layers\",\"5\",\"--model_dir\",\"s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/\",\"--model_filename\",\"model_dl_trf_sm_0\",\"--num_epochs\",\"30\",\"--num_heads\",\"4\",\"--output_dim\",\"1\",\"--patience\",\"5\",\"--seed\",\"42\",\"--transformer_layers\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ATTN_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_FORWARD_DIM=5\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIMS_L=[128,32]\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=9\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LSTM_DIM=64\u001b[0m\n",
      "\u001b[34mSM_HP_LSTM_LAYERS=5\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_FILENAME=model_dl_trf_sm_0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_HEADS=4\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[34mSM_HP_PATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TRANSFORMER_LAYERS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 4 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.9 -m mpi4py dl_train_trf.py --attn_dropout 0.1 --batch_size 12 --dropout 0.3 --forward_dim 5 --hidden_dims_l [128,32] --input_dim 9 --learning_rate 0.0002 --lstm_dim 64 --lstm_layers 5 --model_dir s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/ --model_filename model_dl_trf_sm_0 --num_epochs 30 --num_heads 4 --output_dim 1 --patience 5 --seed 42 --transformer_layers 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Bootstrap : Using eth0:10.0.235.124<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Bootstrap : Using eth0:10.0.235.124<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Bootstrap : Using eth0:10.0.235.124<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Bootstrap : Using eth0:10.0.235.124<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Channel 00/02 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Channel 01/02 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Channel 00 : 3[1e0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Channel 01 : 3[1e0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:128:128 [3] NCCL INFO comm 0x55d97ddd7d90 rank 3 nranks 4 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:122:122 [1] NCCL INFO comm 0x55c84e428ab0 rank 1 nranks 4 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:276:276 [0] NCCL INFO comm 0x55a3b5027920 rank 0 nranks 4 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDP: Single node mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:127:127 [2] NCCL INFO comm 0x56497c7f33c0 rank 2 nranks 4 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:device: cuda\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:cuda:3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:device: cuda\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:cuda:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:device: cuda\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:cuda:1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:device: cuda\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:cuda:0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:val dataloader step passed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:data loaded\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:device 0: model prepared\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:device 1: model prepared\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training start\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 0 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:device 2: model prepared\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:device 3: model prepared\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.748 algo-1:276 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.748 algo-1:122 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.749 algo-1:127 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.749 algo-1:128 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/smexperiments/__init__.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  import pkg_resources\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/smexperiments/__init__.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  import pkg_resources\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/smexperiments/__init__.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  import pkg_resources\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/smexperiments/__init__.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  import pkg_resources\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  declare_namespace(pkg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.973 algo-1:122 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.973 algo-1:122 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.973 algo-1:128 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.973 algo-1:122 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:128 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:122 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:122 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:128 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:276 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:128 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2025-03-03 04:18:07.974 algo-1:128 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:127 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:276 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:276 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:127 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:127 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.975 algo-1:276 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2025-03-03 04:18:07.976 algo-1:276 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.976 algo-1:127 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2025-03-03 04:18:07.976 algo-1:127 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 1/30, Train loss: 0.043\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 1/30, Train loss: 0.043\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 1/30, Train loss: 0.043\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 1/30, Train loss: 0.043\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 1/30, Val loss: 0.023\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 1 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 1/30, Val loss: 0.023\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 1/30, Val loss: 0.023\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 1/30, Val loss: 0.023\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 2/30, Train loss: 0.030\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 2/30, Train loss: 0.030\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 2/30, Train loss: 0.030\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 2/30, Train loss: 0.030\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 2/30, Val loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 2/30, Val loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 2 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 2/30, Val loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 2/30, Val loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 3/30, Train loss: 0.027\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 3/30, Train loss: 0.027\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 3/30, Train loss: 0.027\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 3/30, Train loss: 0.027\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 3/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 3/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 3/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 3 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 3/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 4/30, Train loss: 0.024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 4/30, Train loss: 0.024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 4/30, Train loss: 0.024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 4/30, Train loss: 0.024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 4/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 4/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 4 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 4/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 4/30, Val loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 5/30, Train loss: 0.021\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 5/30, Train loss: 0.021\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 5/30, Train loss: 0.021\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 5/30, Train loss: 0.021\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 5/30, Val loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 5/30, Val loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 5 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 5/30, Val loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 5/30, Val loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 6/30, Train loss: 0.019\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 6/30, Train loss: 0.019\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 6/30, Train loss: 0.019\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 6/30, Train loss: 0.019\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 6/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 6/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 6/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 6/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 6 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 7/30, Train loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 7/30, Train loss: 0.018[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 7/30, Train loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 7/30, Train loss: 0.018\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 7/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 7/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 7/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 7 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 7/30, Val loss: 0.012\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 8/30, Train loss: 0.017\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 8/30, Train loss: 0.017\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 8/30, Train loss: 0.017\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 8/30, Train loss: 0.017\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 8/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 8/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 8/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 8 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 8/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 9/30, Train loss: 0.016\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 9/30, Train loss: 0.016\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 9/30, Train loss: 0.016\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 9/30, Train loss: 0.016\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 9/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 9/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 9/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 9 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 9/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 10/30, Train loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 10/30, Train loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 10/30, Train loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 10/30, Train loss: 0.015\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 10/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 10/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 10 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 10/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 10/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 11/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 11/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 11/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 11/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 11/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 11/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 11/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 11 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 11/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 12/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 12/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 12/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 12/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 12/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 12/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 12/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 12 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 12/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 13/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 13/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 13/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 13/30, Train loss: 0.014\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 13/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 13/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 13 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 13/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 13/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 14/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 14/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 14/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 14/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 14/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 14/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 14 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 14/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 14/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 15/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 15/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 15/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 15/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 15/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 15/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 15 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 15/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 15/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 16/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 16/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 16/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 16/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 16/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 16/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 16/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 16 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 16/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 17/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 17/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 17/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 17/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 17/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 17/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 17 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 17/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 17/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 18/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 18/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 17\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 17\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 18/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 18/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 18/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 18/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 18/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 17\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 18 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 18/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 19/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 19/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 19/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 19/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 18\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 18\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 19/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 19/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 18\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 19 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 19/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 19/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 20/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 20/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 19\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 19\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 20/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 20/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 20/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 20/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 19\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 20 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 20/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 20/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 21/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 21/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 21/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 21/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 21/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 21/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 21/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 21/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 21 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 22/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 22/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 22/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 21\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 21\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 22/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 22/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 22/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 22/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 22/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 21\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 22 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 23/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 23/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 23/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 22\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 22\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 23/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 23/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 23/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 22\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 23 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 23/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 23/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 24/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 24/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 24/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 23\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 23\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 24/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 24/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 24/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 23\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 24 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 24/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 24/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 25/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 25/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 25/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 24\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 24\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 25/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 25/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 24\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 25 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 25/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 25/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 25/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 26/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 26/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 26/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 26/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 25\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 25\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 26/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 25\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 26 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 26/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 26/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 26/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 27/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 26\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 26\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 27/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 27/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 27/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 27/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 26\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 27 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 27/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 27/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 27/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 28/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 28/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 28/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 27\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 27\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 28/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 28/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 27\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 28 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 28/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 28/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 28/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 29/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 29/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 28\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 28\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 29/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 29/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 29/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 29/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 29/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 29/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 28\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:EPOCH 29 START\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Device: 2; Epoch 30/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Device: 0; Epoch 30/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:training end for epoch 29\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation start for epoch 29\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Device: 3; Epoch 30/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Device: 1; Epoch 30/30, Train loss: 0.013\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Epoch 30/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Epoch 30/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Epoch 30/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch 30/30, Val loss: 0.011\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:validation end for epoch 29\u001b[0m\n",
      "\u001b[34m2025-03-03 05:03:07,274 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-03-03 05:03:07,274 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-03-03 05:03:07,274 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2025-03-03 05:03:37,282 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2025-03-03 05:03:37,282 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-03-03 05:03:46 Uploading - Uploading generated training model\n",
      "2025-03-03 05:03:46 Completed - Training job completed\n",
      "Training seconds: 3245\n",
      "Billable seconds: 3245\n",
      "CPU times: user 6.46 s, sys: 354 ms, total: 6.81 s\n",
      "Wall time: 56min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# call the estimator\n",
    "s3_folder = 's3://capstone-general/NN-related/dl_model/'\n",
    "model_checkpoint = 'dl_model_checkpoint0'\n",
    "rebal_freq = 'D'\n",
    "model_content_full_path = f'{s3_folder}{model_checkpoint}/{rebal_freq}/'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"dl_train_trf.py\",\n",
    "    role=role,\n",
    "    dependencies=[\"requirements.txt\"], # !CHANGE! (separately) create req.txt from dependencies\n",
    "    py_version=\"py39\", #? python version ## ELAINE\n",
    "    framework_version=\"1.13.1\", #pytorch version ## ELAINE\n",
    "    instance_count=1,\n",
    "    # instance_type=\"ml.c5.xlarge\",\n",
    "    # instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    hyperparameters={\n",
    "        'model_dir': model_content_full_path,\n",
    "        'model_filename': 'model_dl_trf_sm_0',\n",
    "        'batch_size': 12,  ## ELAINE - SET AS 1/4 DESIRED BATCH SIZE FOR GRAD ACCUMULATION\n",
    "        'learning_rate': 0.0002,\n",
    "        'input_dim': 9,\n",
    "        'lstm_dim': 64,\n",
    "        'lstm_layers': 5,\n",
    "        'transformer_layers': 3,\n",
    "        'num_heads': 4,\n",
    "        'attn_dropout': 0.1,\n",
    "        'forward_dim': 5,\n",
    "        'hidden_dims_l':'[128,32]',\n",
    "        'dropout': 0.3,\n",
    "        'output_dim': 1,\n",
    "        'num_epochs': 30,\n",
    "        'patience': 5,\n",
    "        'seed': 42\n",
    "    },\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},  ## ELAINE\n",
    ")\n",
    "# output_path=\"s3://capstone-general/text-models/output\", # !CHANGE! where to put model.tar.gz\n",
    "\n",
    "# Start training\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/models/model.pth to ./model.pth\n"
     ]
    }
   ],
   "source": [
    "# get model from bucket\n",
    "!aws s3 cp s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/models/model.pth ./model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Create the tar.gz file\n",
    "with tarfile.open('model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add('model.pth')  # Add the model.pth inside the tar.gz\n",
    "    tar.add('inference.py')\n",
    "    tar.add('requirements.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place model.tar.gz in bucket\n",
    "!aws s3 cp /model.tar.gz s3://capstone-general/NN-related/dl_model/dl_model_checkpoint0/D/models/model.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results from estomator for later processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO using sagemaker - not attempted 2025-03-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "\n",
    "# Define the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'batch_size': IntegerParameter(24, 128),\n",
    "    'learning_rate': ContinuousParameter(0.00005, 0.0005),\n",
    "    'lstm_dim': IntegerParameter(48, 128),\n",
    "    'lstm_layers': IntegerParameter(2, 12),\n",
    "    'transformer_layers': IntegerParameter(1, 10),\n",
    "    'num_heads': IntegerParameter(2, 10),\n",
    "    'att_dropout': ContinuousParameter(0.1, 0.6),\n",
    "    'forward_dim': IntegerParameter(2, 10),\n",
    "    # 'hidden_dims_l': [128,32],\n",
    "    'dropout': ContinuousParameter(0.4, 0.6),\n",
    "    'hidden_dim': IntegerParameter(64, 256),\n",
    "    'dropout': ContinuousParameter(0.1, 0.5)\n",
    "}\n",
    "\n",
    "# Create the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='validation:L1Loss',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Launch the hyperparameter tuning job\n",
    "tuner.fit()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "w5Gxx606sJBu",
    "GVCmyztGsJCI",
    "_kblEA-osJDF"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
