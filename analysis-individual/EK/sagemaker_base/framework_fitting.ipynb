{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de9875f-8e83-4ab4-996a-b82276147811",
   "metadata": {},
   "source": [
    "# Sagemaker Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963ec75-546e-4585-b971-c75eb9ee6204",
   "metadata": {},
   "source": [
    "## Environment and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8facc3ea-d8d9-4c86-a88a-4bf85aecd96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 05:29:50.591418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# import as needed\n",
    "# if only needed in script, does not need to be here\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "### sagemaker essentials  - copy ###\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch # as needed framework\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# sagemaker initiation\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()  \n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# extra\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f55bd77e-18d5-48f5-a6a9-c138d09182ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    }
   ],
   "source": [
    "# upload data beforehand\n",
    "# data should be in bucket\n",
    "df = pd.read_parquet('s3://capstone-general/text-data/FNSPID_NYT_Combined_Dataset_021125.parquet')\n",
    "market = pd.read_pickle(\"s3://capstone-general/NN-related/data_checkpoint1/mkt_daily.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af6c18-9ecb-44d8-94c7-19e422ccbd0b",
   "metadata": {},
   "source": [
    "## Pre-training Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169cb26-2016-47a1-aa0e-6d490c1e04ee",
   "metadata": {},
   "source": [
    "This is straight from a Colab notebook. Data splitting can be done in the script too (personal preference). If it is done in the script there is no need to pass paths later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82589674-8bf1-4b62-93ff-6bc546b73b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Shift market returns for prediction alignment (next-day returns)\n",
    "market[\"return_sp_lag\"] = market[\"return_sp\"].shift(-1)\n",
    "\n",
    "# Merge market data with news data\n",
    "df = df.merge(market[['return_sp_lag']], left_on=\"Date\", right_index=True, how=\"left\")\n",
    "\n",
    "# Clean and prepare the dataframe\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "# df = df[[\"Date\", \"Summary\", \"return_sp_lag\"]].dropna()\n",
    "\n",
    "print(\"✅ Data loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eeb075d0-dc91-47bf-8d99-700cf9f9f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Size: 1237400 | Validation Size: 1427442\n"
     ]
    }
   ],
   "source": [
    "# Train-Validation Split (Train: <= Dec 31, 2019 | Validation: >= Jan 1, 2020)\n",
    "train_df = df[df[\"Date\"] <= \"2020-12-31\"].reset_index(drop=True)\n",
    "val_df = df[df[\"Date\"] > \"2020-12-31\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Train Size: {len(train_df)} | Validation Size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53424a4-fa77-41f8-81be-3909a36832e1",
   "metadata": {},
   "source": [
    "## Framework Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d9a4a-326d-44ec-a041-57fa29284a33",
   "metadata": {},
   "source": [
    "If splitting outside of training script: We need to save the train and test sets (for convenience), and know where in the bucket they are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826c7768-4345-4e58-83c1-8f77140cf915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_path = \"s3://capstone-general/text-data/train/\"\n",
    "val_s3_path = \"s3://capstone-general/text-data/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "677fafc1-fac0-42b6-bf75-8c5f8db57343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet for safekeeping, keep paths in mind\n",
    "train_df.to_parquet(f\"{train_s3_path}train.parquet\", index=False)\n",
    "val_df.to_parquet(f\"{val_s3_path}val.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75018cf-9b56-4688-ac11-e8f0c3882ce6",
   "metadata": {},
   "source": [
    "Below is the framework for beginning a training job. The lines which may be good to change are commented with their purpose.\n",
    "\n",
    "The most important fields are the following:\n",
    "- dependencies\n",
    "    - Any package that is not in the Python default library should be put into a requirements.txt which will be installed at the script initiation.\n",
    "- instance_type\n",
    "    - In the process of testing the training job to see if it will work, the first error that may be encountered is a dependency error, thus the requirements.txt. However, failed job seconds still count as runtime seconds, so it is preferred to choose a non-GPU instance (ml.t3.large, etc) during the first run to check for the dependencies. The run will fail quickly, but should fail as a \"GPU needed\" error and not a dependency error.\n",
    "    - Additionally, it is generally better to start with a smaller instance and work up with need. Generally if GPU memory is the limiting factor, I would recommend this path, unless you already have a decent idea how much memory you need:\n",
    "        - ml.g4dn.xlarge (16G) -> ml.g4dn.12xlarge/ml.p3.2xlarge (4x16G // V100) -> ml.p4d.24xlarge (8x40G or A100)\n",
    "- hyperparameters\n",
    "    - This is a dictionary to define hyperparameters from \"outside\", relative to the training script (\"inside\"). The values can be anything and any length, as long as there is a corresponding argparse field in the training script.\n",
    "- output_path\n",
    "    - This is a reference to where the model.tar.gz file will be placed.\n",
    "- environment\n",
    "    - This is also a dictionary, for environment variables. I have these as variables which are not hyperparameters but could potentially need to be changed easily without opening train.py. Also, I was struggling decently to get the job to find the data from the fit call, so I recommend setting these at least anyway. I don't think they need to be a particular name or within a certain set, as you read them with os.environ, so they can be anything you deem necessary.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae9e9e-23db-4eac-813b-fd18951e69ff",
   "metadata": {},
   "source": [
    "After filling the object initiator out, we can call .fit() which will begin the training job. The paths to the train and test data should be passed as a dictionary (and these variables need to be enclosed as arguments in train.py with argparse). Follow the logs to understand the general flow of the job. You can also go to the Studio page (from where your notebook was launched) and click on Jobs > Training Jobs on the left. This is a convenient way to check the progress, logs, environment variables, hyperparameters, etc of the job, or to kill it. \n",
    "\n",
    "Note, this notebook and the training job run on separate instances (that's why you can pick the instance type for both). Therefore the training job is not dependent on this notebook's runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e30523-bcb3-4e1b-aa35-21aa5df7bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-02-27-05-03-19-990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 05:04:08 Starting - Starting the training job...\n",
      "2025-02-27 05:04:31 Starting - Preparing the instances for training...\n",
      "2025-02-27 05:04:59 Downloading - Downloading input data...\n",
      "2025-02-27 05:05:29 Downloading - Downloading the training image..................\n",
      "2025-02-27 05:08:40 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-02-27 05:08:53,071 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-02-27 05:08:53,094 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-02-27 05:08:53,109 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:08:53,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:08:56,123 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.17.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.9/67.9 kB 3.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.17.0->-r requirements.txt (line 1)) (3.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.17.0->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.17.0->-r requirements.txt (line 1)) (24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.17.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.17.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 5.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.17.0->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses (from transformers==4.17.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,>=0.11.1 (from transformers==4.17.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2024.6.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<6.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.17.0->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.17.0->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.17.0->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.17.0->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0->-r requirements.txt (line 1)) (8.1.7)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 82.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.4/485.4 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 100.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 780.9/780.9 kB 75.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 127.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 897.5/897.5 kB 78.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.9/193.9 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.9/242.9 kB 31.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.1/124.1 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208.5/208.5 kB 26.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.5/321.5 kB 36.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, regex, propcache, multidict, frozenlist, async-timeout, aiohappyeyeballs, yarl, sacremoses, aiosignal, tokenizers, aiohttp, transformers, datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 frozenlist-1.5.0 multidict-6.1.0 propcache-0.3.0 regex-2024.11.6 sacremoses-0.1.1 tokenizers-0.21.0 transformers-4.17.0 xxhash-3.5.0 yarl-1.18.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,146 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,146 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,197 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,248 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,300 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,321 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"learning_rate\": 0.02,\n",
      "        \"max_grad_norm\": 2.0,\n",
      "        \"warmup_ratio\": 0.2,\n",
      "        \"weight_decay\": 0.02\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2025-02-27-05-03-19-990\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://capstone-general/pytorch-training-2025-02-27-05-03-19-990/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"gradient_accumulation_steps\":4,\"learning_rate\":0.02,\"max_grad_norm\":2.0,\"warmup_ratio\":0.2,\"weight_decay\":0.02}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://capstone-general/pytorch-training-2025-02-27-05-03-19-990/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"gradient_accumulation_steps\":4,\"learning_rate\":0.02,\"max_grad_norm\":2.0,\"warmup_ratio\":0.2,\"weight_decay\":0.02},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2025-02-27-05-03-19-990\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://capstone-general/pytorch-training-2025-02-27-05-03-19-990/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--gradient_accumulation_steps\",\"4\",\"--learning_rate\",\"0.02\",\"--max_grad_norm\",\"2.0\",\"--warmup_ratio\",\"0.2\",\"--weight_decay\",\"0.02\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.02\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=2.0\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.02\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 2 --gradient_accumulation_steps 4 --learning_rate 0.02 --max_grad_norm 2.0 --warmup_ratio 0.2 --weight_decay 0.02\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:04,363 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Parsed arguments: Namespace(pretrained_model='yiyanghkust/finbert-tone', train='/opt/ml/input/data/train', val='/opt/ml/input/data/val', output_dir='/opt/ml/model', epochs=2, weight_decay=0.02, gradient_accumulation_steps=4, learning_rate=0.02, warmup_ratio=0.2, max_grad_norm=2.0)\u001b[0m\n",
      "\u001b[34maggregating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mtext length:  1\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/221k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 193k/221k [00:00<00:00, 1.41MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 221k/221k [00:00<00:00, 1.60MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/533 [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 533/533 [00:00<00:00, 2.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/419M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 7.46M/419M [00:00<00:05, 78.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 17.3M/419M [00:00<00:04, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 27.2M/419M [00:00<00:04, 97.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 37.1M/419M [00:00<00:03, 100MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 46.9M/419M [00:00<00:03, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▎        | 56.8M/419M [00:00<00:03, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 66.6M/419M [00:00<00:03, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 76.5M/419M [00:00<00:03, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 86.5M/419M [00:00<00:03, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 96.4M/419M [00:01<00:03, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 106M/419M [00:01<00:03, 104MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 116M/419M [00:01<00:03, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 126M/419M [00:01<00:02, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 136M/419M [00:01<00:02, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 146M/419M [00:01<00:02, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 156M/419M [00:01<00:02, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 166M/419M [00:01<00:02, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 176M/419M [00:01<00:02, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 186M/419M [00:01<00:02, 105MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 196M/419M [00:02<00:02, 106MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 207M/419M [00:02<00:02, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 217M/419M [00:02<00:01, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 228M/419M [00:02<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 238M/419M [00:02<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 248M/419M [00:02<00:01, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 259M/419M [00:02<00:01, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 269M/419M [00:02<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 280M/419M [00:02<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 290M/419M [00:02<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 300M/419M [00:03<00:01, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 311M/419M [00:03<00:01, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 321M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 332M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 342M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 353M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 363M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 374M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 384M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 394M/419M [00:03<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 405M/419M [00:04<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 415M/419M [00:04<00:00, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 419M/419M [00:04<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34maggregating embeddings:  33%|███▎      | 1/3 [00:09<00:18,  9.18s/it]\u001b[0m\n",
      "\u001b[34mtext length:  1\u001b[0m\n",
      "\u001b[34maggregating embeddings:  67%|██████▋   | 2/3 [00:11<00:05,  5.11s/it]\u001b[0m\n",
      "\u001b[34mtext length:  1\u001b[0m\n",
      "\u001b[34maggregating embeddings: 100%|██████████| 3/3 [00:13<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34maggregating embeddings: 100%|██████████| 3/3 [00:13<00:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34maggregating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mtext length:  66\u001b[0m\n",
      "\u001b[34maggregating embeddings:  33%|███▎      | 1/3 [00:02<00:05,  2.63s/it]\u001b[0m\n",
      "\u001b[34mtext length:  59\u001b[0m\n",
      "\u001b[34maggregating embeddings:  67%|██████▋   | 2/3 [00:05<00:02,  2.71s/it]\u001b[0m\n",
      "\u001b[34mtext length:  111\u001b[0m\n",
      "\u001b[34maggregating embeddings: 100%|██████████| 3/3 [00:08<00:00,  2.93s/it]\u001b[0m\n",
      "\u001b[34maggregating embeddings: 100%|██████████| 3/3 [00:08<00:00,  2.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Progress: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.331 algo-1:80 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.561 algo-1:80 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.562 algo-1:80 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.562 algo-1:80 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.563 algo-1:80 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-02-27 05:09:36.563 algo-1:80 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.67822265625, 'eval_accuracy': 0.6666666666666666, 'eval_runtime': 0.315, 'eval_samples_per_second': 9.524, 'eval_steps_per_second': 3.175, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 5.19921875, 'eval_accuracy': 0.0, 'eval_runtime': 0.3166, 'eval_samples_per_second': 9.477, 'eval_steps_per_second': 3.159, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1.8678, 'train_samples_per_second': 3.212, 'train_steps_per_second': 1.071, 'train_loss': 0.17071533203125, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mTraining Progress: 0it [00:01, ?it/s]\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:38,881 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:38,881 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-02-27 05:09:38,882 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-02-27 05:09:59 Uploading - Uploading generated training model\n",
      "2025-02-27 05:09:59 Completed - Training job completed\n",
      "Training seconds: 300\n",
      "Billable seconds: 300\n"
     ]
    }
   ],
   "source": [
    "estimator = PyTorch( # !CHANGE! appropriate framework (HuggingFace if out of box)\n",
    "    entry_point=\"train.py\", # !CHANGE! training script\n",
    "    source_dir=\".\", # !CHANGE! reference path to files (e.g. requirements.txt, train.py). can and likely should be an s3 path (\"s3:// ... /source-code/\")\n",
    "    role=role, # sagemaker execution role\n",
    "    dependencies=[\"requirements.txt\"], # !CHANGE! (separately) create req.txt from dependencies\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # !CHANGE! recommended to check for dependency errors with small instance (t, m, c) first. it will fail but you avoid running seconds for script package dependencies\n",
    "    instance_count=1, # increase for torch.nn.parallel.DistributedDataParallel\n",
    "    framework_version=\"1.13\", # pytorch/etc version\n",
    "    py_version=\"py39\", \n",
    "    hyperparameters={\"epochs\": 2,\n",
    "                     \"weight_decay\" : 0.02,\n",
    "                     \"gradient_accumulation_steps\" : 4,\n",
    "                     \"learning_rate\" :0.02,\n",
    "                     \"warmup_ratio\" : 0.2,\n",
    "                     \"max_grad_norm\" : 2.0\n",
    "                    }, #!CHANGE! script calls hyperparameters as arguments\n",
    "    output_path=\"s3://capstone-general/text-models/output\", # !CHANGE! where to put model.tar.gz\n",
    "    input_mode=\"File\",\n",
    "    environment={\"TOKENIZERS_PARALLELISM\": \"false\", # Avoid tokenizer parallelism warning (chatgpt put this)\n",
    "                 \"SM_MODEL_DIR\":\"s3://capstone-general/text-models/output\", # to set default/expected output directory\n",
    "                 \"SM_CHANNEL_TRAIN\": train_s3_path, # to set default/expected train directory\n",
    "                 \"SM_CHANNEL_VAL\": val_s3_path, # to set default/expected val directory\n",
    "                 \"BASE_MODEL\": \"yiyanghkust/finbert-tone\"},  # to set default/expected model\n",
    ")\n",
    "\n",
    "# Start training\n",
    "estimator.fit({\"train\": train_s3_path, \"val\": val_s3_path}) # !CHANGE! the training script takes train/val directories as arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559478f-d01f-40f8-b23f-4798906e3ac1",
   "metadata": {},
   "source": [
    "After training is successful, check the S3 bucket path.\n",
    "\n",
    "One thing you should notice in the bucket root is that potentially, there are many many folders named \"pytorch-training-YYYY-MM-DD-HH-MM-SS...\", depending on however many training jobs were *initiated* (includes fails). They have \"sourcedir.tar.gz\" in them. This isn't relevant to us, but it takes up a lot of storage, so if you have a lot of trial and error before you get a success, once you ARE successful go to the bucket and delete the failed previous attempts.\n",
    "\n",
    "In the specified output path, there should be a folder with an identical name (\"pytorch-training...\"), inside of which includes source/model.tar.gz. Note this path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdca568-99dc-43cb-b758-4bbcc76e5b0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HF Template (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd188b7-6e15-4c61-9732-3bbf0cb2808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'yiyanghkust/finbert-tone',\n",
    "\t'HF_TASK':'text-classification'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n",
    "\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"I like you. I love you\",\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
