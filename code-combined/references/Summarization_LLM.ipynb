{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db85efb0d2314b629286da89fbb3a04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46cff652aecd4ab2b84c801d4541ad5f",
              "IPY_MODEL_ccd2c1cf239e4bd780d045a6cd136db2",
              "IPY_MODEL_7ee2b56deab240e8986ae1b352df3a4f"
            ],
            "layout": "IPY_MODEL_a41a0e448cc1422ab4d897e5679087eb"
          }
        },
        "46cff652aecd4ab2b84c801d4541ad5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110c7d2d46214bfcbf9f9877c60cafa2",
            "placeholder": "​",
            "style": "IPY_MODEL_eb9f299f7480494d8516a2e142f482fe",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ccd2c1cf239e4bd780d045a6cd136db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c35e98c23f43faaac7887cc3a20617",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eec2373b92454b8fb8c654bad013ca82",
            "value": 4
          }
        },
        "7ee2b56deab240e8986ae1b352df3a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df8ba627610481096c22fff3ff4726e",
            "placeholder": "​",
            "style": "IPY_MODEL_5d3db03c6f2b414a8b78ec225f4aa12a",
            "value": " 4/4 [01:24&lt;00:00, 18.11s/it]"
          }
        },
        "a41a0e448cc1422ab4d897e5679087eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "110c7d2d46214bfcbf9f9877c60cafa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9f299f7480494d8516a2e142f482fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1c35e98c23f43faaac7887cc3a20617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eec2373b92454b8fb8c654bad013ca82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9df8ba627610481096c22fff3ff4726e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d3db03c6f2b414a8b78ec225f4aa12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMa 3.1\n",
        "\n",
        "LLaMa 3.1 is a very recent model from Meta. Check out [the model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) for further details. It is open-sourced.  To use it, you need to log in to your Hugging Face account and get permission.  We're using the 8 billion parameter version but quantized so it has a much smaller memory footprint.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2024-fall-main/blob/master/materials/lesson_notebooks/lesson_7_summarization_LLM.ipynb)"
      ],
      "metadata": {
        "id": "y9eZI4y7U88b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes flash_attn"
      ],
      "metadata": {
        "id": "JDr0c9pgi5Qa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "7IMPokHgtffJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's some text from the introduction to [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/pdf/2406.06608).  Let's have the model summarize it."
      ],
      "metadata": {
        "id": "OWwuz03eAwoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE = \"Scope of Study We create a broad directory of prompting techniques, which can be quickly understood and easily implemented for rapid experimentation by developers and researchers. To this end, we limit our study to focus on discrete prefix prompts (Shin et al., 2020a) rather than cloze prompts (Petroni et al., 2019; Cui et al., 2021), because modern LLM architectures (especially decoder-only models), which use prefix prompts, are widely used and have robust support for both consumers and researchers. Additionally, we refined our focus to hard (discrete) prompts rather than soft (continuous) prompts and leave out papers that make use of techniques using gradient-based updates (i.e. fine-tuning). Finally, we only study task-agnostic techniques. These decisions keep the work approachable to less technical readers and maintain a manageable scope. \"\n",
        "\n",
        "ARTICLE += \"Sections Overview We conducted a machine-assisted systematic review grounded in the PRISMA process (Page et al., 2021) (Section 2.1) to identify 58 different text-based prompting techniques, from which we create a taxonomy with a robust terminology of prompting terms (Section 1.2) While much literature on prompting focuses on English-only settings, we also discuss multilingual techniques (Section 3.1). Given the rapid growth in multimodal prompting, where prompts may include media such as images, we also expand our scope to multimodal techniques (Section 3.2). Many multilingual and multimodal prompting techniques are direct extensions of English text-only prompting techniques. \"\n",
        "\n",
        "ARTICLE += \"As prompting techniques grow more complex, they have begun to incorporate external tools, such as Internet browsing and calculators. We use the term ‘agents‘ to describe these types of prompting techniques (Section 4.1). It is important to understand how to evaluate the outputs of agents and prompting techniques to ensure accuracy and avoid hallucinations.\"\n",
        "\n",
        "len(ARTICLE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo3IBfof2a49",
        "outputId": "a3fbe9fe-f554-4a1b-99e2-487334b92cd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1898"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "h2JdB4kcjHF9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the LLama 3.1 model using the Pipeline abstraction from Hugging Face and before we have it summarize, let's ask it a question and see how well it answers.  Do you think the answer is accurate?  How might we evaluate the generated answer?"
      ],
      "metadata": {
        "id": "kz_4_94-BbPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import transformers\n",
        "#import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"quantization_config\": quantization_config},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a science communicator who makes technology accessible to everyone!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Please write a five sentence explanation of how LLMs do knowledge representation.\"},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "pprint(outputs[0][\"generated_text\"][-1], compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "db85efb0d2314b629286da89fbb3a04b",
            "46cff652aecd4ab2b84c801d4541ad5f",
            "ccd2c1cf239e4bd780d045a6cd136db2",
            "7ee2b56deab240e8986ae1b352df3a4f",
            "a41a0e448cc1422ab4d897e5679087eb",
            "110c7d2d46214bfcbf9f9877c60cafa2",
            "eb9f299f7480494d8516a2e142f482fe",
            "e1c35e98c23f43faaac7887cc3a20617",
            "eec2373b92454b8fb8c654bad013ca82",
            "9df8ba627610481096c22fff3ff4726e",
            "5d3db03c6f2b414a8b78ec225f4aa12a"
          ]
        },
        "id": "CPYJUPmBU-no",
        "outputId": "d459cf6f-de36-42f5-a123-185109295a96"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db85efb0d2314b629286da89fbb3a04b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': 'Large Language Models (LLMs) represent knowledge through a '\n",
            "            'complex network of interconnected nodes, where each node '\n",
            "            'corresponds to a unique concept or piece of information. This '\n",
            "            'network, known as a knowledge graph, is built by training the '\n",
            "            'model on vast amounts of text data, allowing it to learn '\n",
            "            'relationships and patterns between concepts. As the model '\n",
            "            'processes new input, it generates a set of \"embeddings\" – '\n",
            "            'numerical vectors that capture the semantic meaning of the input '\n",
            "            'and its connections to other concepts in the graph. These '\n",
            "            'embeddings enable the model to reason about the input, make '\n",
            "            'predictions, and generate responses that reflect its '\n",
            "            'understanding of the underlying knowledge. By leveraging this '\n",
            "            'knowledge graph, LLMs can represent and manipulate complex '\n",
            "            'knowledge structures, such as hierarchies, relationships, and '\n",
            "            \"entities, in a way that's both flexible and scalable.\",\n",
            " 'role': 'assistant'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try it for abstractive summarization.  Note that it takes a lot longer to generate answers because this model has 8 billion.  The next cell can take up to 2 minutes to complete.\n",
        "\n",
        "How good is the output from Llama3.1?  How can we measure the performance? What's are all of the elements we need to say run ROUGE?"
      ],
      "metadata": {
        "id": "zFjMNa0w2AOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert on natural language processing.  Please summarize the following content for a fifth grader. Your summary should be no longer than five sentences.\"},\n",
        "            {\"role\": \"user\", \"content\": ARTICLE},\n",
        "]\n",
        "\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "#lets set some values to have more control over the output\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUqNgfpIVATV",
        "outputId": "90c3ed8d-535c-4510-b54a-a96a2363cb4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Imagine you have a super smart computer program that can understand and '\n",
            " 'respond to questions. To help it understand what we want to do, we give it '\n",
            " 'special instructions called \"prompts.\" We\\'re studying different ways to '\n",
            " \"create these prompts so we can understand how to make them better. We're \"\n",
            " 'focusing on simple prompts that use words, not pictures or math problems. '\n",
            " \"We're also looking at how to make these prompts work in different languages \"\n",
            " 'and with different tools, like the internet or calculators. This will help '\n",
            " \"us make sure the computer program gives us accurate answers and doesn't make \"\n",
            " \"up things that aren't true.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try it yourself.  You can fill in the system and the user portion of the prompt.  See what kinds of questions it can answer and see how well it summarizes content."
      ],
      "metadata": {
        "id": "vZADKJf1-B_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Your Value Here\"},\n",
        "            {\"role\": \"user\", \"content\": \"Your Value Here\"},\n",
        "]\n",
        "\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "#lets set some values to have more control over the output\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
      ],
      "metadata": {
        "id": "mceHEu4A95Iv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}